{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%run \"../../../../PyUtils/common.ipynb\"\n",
    "import sys\n",
    "import importlib as imp\n",
    "#if ('Jupytils' in sys.modules):\n",
    "#    reloaded = imp.reload(Jupytils)\n",
    "#else:\n",
    "#    import Jupytils\n",
    "    \n",
    "    \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcess\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "patient = \"hand2\"\n",
    "ampm = \"am\"\n",
    "file = \"http://www.smartconnectedhealth.org/aura/webroot/db.jsp?q=SELECT%20*%20FROM%20calculated_jlee%20WHERE%20ampm=%27\"+ampm+\"%27%20AND%20pname=%27\"+patient+\"%27%20ORDER%20BY%20date,%20time\"\n",
    "\n",
    "lowerPercent = 0.25\n",
    "upperPercent = 0.75\n",
    "\n",
    "fileName = file;\n",
    "\n",
    "dfOriginal = LoadDataSet(fileName, checkForDateTime=False);\n",
    "#displayDFs (dfOriginal, maxrows=3 )\n",
    "#for c in dfOriginal.columns: print (c,  end=', ')\n",
    "dfOriginal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break into groups by percentage\n",
    "\n",
    "dfOriginal.sort_values(by='pef', ascending=True, inplace=True)\n",
    "dfOriginal.reset_index(drop=True, inplace=True)\n",
    "numberOfElements = len(dfOriginal)\n",
    "upperBoundary = upperPercent*numberOfElements\n",
    "lowerBoundary = lowerPercent*numberOfElements\n",
    "dfOriginal['percent_group']=-1\n",
    "\n",
    "for i in dfOriginal.index:\n",
    "\n",
    "    if i < lowerBoundary:\n",
    "        dfOriginal['percent_group'][i] = 0\n",
    "\n",
    "    elif i > upperBoundary : \n",
    "        dfOriginal['percent_group'][i] = 2\n",
    "\n",
    "    else:\n",
    "        dfOriginal['percent_group'][i] = 1\n",
    "\n",
    "print(dfOriginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df=dfOriginal.copy()\n",
    "\n",
    "#1. Remove all the rows that does not have any pef values \n",
    "\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "#2. Combine \n",
    "df=df.fillna(0)\n",
    "sdttm = df.date + \" \" + df.time\n",
    "df.insert(0, 'sdttm', sdttm)\n",
    "df.sdttm = pd.to_datetime(df.sdttm)\n",
    "\n",
    "df.sort_values(by='sdttm', ascending=True, inplace=True)\n",
    "drps  = \"pef_zone,cname, pname, date, time, ampm, timeofday, dateofmeasure, npt, pef1, pef2, pef3, temperature_diff, airpressure_diff, precipitation_diff, tm_window\".split(', ')\n",
    "df=df.drop(drps, axis=1, errors='ignore')\n",
    "df=df.reset_index(drop=True)\n",
    "\n",
    "## <== do the following for SAP PA tool\n",
    "#pef = df.pef;\n",
    "#df=df.drop(['pef'], axis=1)\n",
    "#df.insert(1, 'pef1', pef)\n",
    "\n",
    "#pef[-10:]=0     # Set last 10 to zero for predictions\n",
    "#df.insert(1, 'pef', pef)\n",
    "#df.to_csv(\"HanD/hand1.csv\", sep=';')\n",
    "\n",
    "df1 = df.copy()\n",
    "df1 = df1.set_index(df1.sdttm)\n",
    "df1=df1.drop('sdttm', axis=1, errors='ignore')\n",
    "\n",
    "nq = 5\n",
    "lnq=list(range(0,nq))\n",
    "s= pd.qcut(df1.pef, nq, labels=lnq )\n",
    "df1.insert(1,\"pefcat\", s)\n",
    "df1.pefcat = df1.pefcat.astype(str)\n",
    "\n",
    "\n",
    "\n",
    "pefcol = df1['pef']\n",
    "\n",
    "\n",
    "columns =  '''so2_max,co_max,o3_max,no2_max,pm10_max,so2_sum,co_sum,o3_sum,no2_sum,pm10_sum,so2_avg,co_avg,o3_avg,no2_avg,pm10_avg,temperaturec,windspeedms,winddirection,precipitationpercent,vaporpressurehpa,dewpointtemperaturec,airpressurehpa,sealevelpressurehpa,groundtemperaturec,\n",
    " temperature_max,airpressure_max,precipitation_max,temperature_min,airpressure_min,precipitation_min,percent_group'''\n",
    "\n",
    "#'''pefcat,pef_zone,so2_max,co_max,o3_max,no2_max,temperaturec,windspeedms,precipitationpercent,vaporpressurehpa,\n",
    "#dewpointtemperaturec,airpressurehpa,sealevelpressurehpa,groundtemperaturec,temperature_max,airpressure_max,temperature_min'''\n",
    "\n",
    " \n",
    "#amin,pmin,tmaxlesstmin,amaxlessamin,pmaxlesspmin'''\n",
    "cs = [c.strip() for c in columns.split(',')]\n",
    "\n",
    "df1=df1[cs]\n",
    "#HTML(df1.to_html())\n",
    "#displayDFs (df1, maxrows=3 )\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "# clf = nn;\n",
    "\n",
    "# Y_SCALER = None\n",
    "# #Normalize the input and save the scalar for outcome variable \"pefmax\" in this case.\n",
    "# scaler = StandardScaler()\n",
    "# dp = prepareDF(df1, True)\n",
    "# for c in dp.columns:\n",
    "#     if ( c == 'pefcat'):\n",
    "#         #Y_SCALER = StandardScaler()\n",
    "#         #dp[c] = Y_SCALER.fit_transform(dp[c])\n",
    "#         pass;\n",
    "#     else:\n",
    "#         dp[c] = scaler.fit_transform(dp[c])\n",
    "        \n",
    "# #dp = dp[\"so2,pef,o3,co,no2,pm10,tmax\".split(',')]\n",
    "# dp1 = dp.set_index(pd.Series(range(0,len(dp)) ) )\n",
    "\n",
    "# X = dp1.ix[:, dp1.columns != 'pefcat']\n",
    "# y = dp1.pefcat \n",
    "# X = X.as_matrix().astype(np.float)\n",
    "\n",
    "\n",
    "# kf = sklearn.model_selection.KFold(n_splits=5, shuffle=True)\n",
    "# y_pred = y.copy()\n",
    "\n",
    "# # Iterate through folds\\\n",
    "# i = 0;\n",
    "# for train_index, test_index in kf.split(X):\n",
    "#     #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "#     X_train, X_test = X[train_index], X[test_index]\n",
    "#     y_train = y[train_index]\n",
    "#     # Initialize a classifier with key word arguments\\\n",
    "#     print (\"*\",i, end =\"\");\n",
    "#     clf.fit(X_train,y_train)\n",
    "#     y_pred[test_index] = clf.predict(X_test)\n",
    "#     i = i +1;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "predictColumn = \"pefcat\"\n",
    "predictColumn = \"pef_zone\"\n",
    "predictColumn = \"percent_group\"\n",
    "\n",
    "if ( 'pefcat' in df1.columns ):\n",
    "    df1.pefcat = df1.pefcat.astype(int)\n",
    "    df1=df1.drop('pefcat', axis=1)\n",
    "\n",
    "cls = [# Comment/uncomment out any classfiers according to your need \n",
    "       # DO NOT CHANGE\n",
    "        \"DecisionTree Gini\" , tree.DecisionTreeClassifier(max_depth=4, criterion=\"gini\"),\n",
    "        \"DecisionTree Entr\" , tree.DecisionTreeClassifier(max_depth=4, criterion=\"entropy\"), \n",
    "        \"SVM\"               , sklearn.svm.SVC(kernel=\"linear\"), \n",
    "#        \"SVM1\"              , SVC(kernel=\"rbf\", C=10**3),\n",
    "        \"Random Forest\"     , sklearn.ensemble.RandomForestClassifier(),\n",
    "        \"K-NN\"              , sklearn.neighbors.KNeighborsClassifier(),\n",
    "        \"Gradient Boosting\" , sklearn.ensemble.RandomForestClassifier(),\n",
    "        \"Logit Regression\"  , sklearn.linear_model.LogisticRegression(),\n",
    "#        \"Neural NW\"         , MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(25, 10), random_state=1),\n",
    "        \"Naive Bayes\"       , GaussianNB()\n",
    "];\n",
    "\n",
    "y = df1[predictColumn]\n",
    "X = df1.drop(predictColumn, axis=1)\n",
    "a = Classify(X,y, True, drawConfusionMatrix=True, classifiers=cls, scale=True)\n",
    "\n",
    "\n",
    "ik = 0\n",
    "y_preds=[]\n",
    "clsNames=[]\n",
    "\n",
    "for i in cls:\n",
    "    if type(i) is str:\n",
    "        clsNames.append(i)\n",
    "        y_pred = a[4][i]\n",
    "        y_preds.append(y_pred)\n",
    "        target_names = ['class 0', 'class 1', 'class 2']\n",
    "        print(i + ' Metrics')\n",
    "        y.replace(to_replace=\"R\", value=0, inplace=True, limit=None, regex=False, method='pad', axis=None)\n",
    "        y.replace(to_replace=\"Y\", value=1, inplace=True, limit=None, regex=False, method='pad', axis=None)\n",
    "        y.replace(to_replace=\"G\", value=1, inplace=True, limit=None, regex=False, method='pad', axis=None)\n",
    "        print(classification_report(y, y_pred, target_names=target_names))\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(X1,y1,ra,cls) = Classify1(X,y, True, drawConfusionMatrix=True, classifiers=cls, scale=False)\n",
    "DrawDecisionTree(X,y, cls[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#print(size(pefcol.values))\n",
    "#print(size(y))\n",
    "#print(y_pred)\n",
    "\n",
    "plt.plot(pefcol.index[y == 0],pefcol.values[y == 0],'r.',pefcol.index[y == 1],pefcol.values[y == 1],'y.',pefcol.index[y == 2],pefcol.values[y == 2],'g.')\n",
    "\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('pef')\n",
    "plt.title('PEF true')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "i=0\n",
    "\n",
    "for ypredicted in y_preds:\n",
    "        plt.plot(pefcol.index[ypredicted == 0],pefcol.values[ypredicted == 0],'r.',pefcol.index[ypredicted == 1],pefcol.values[ypredicted == 1],'y.',pefcol.index[ypredicted == 2],pefcol.values[ypredicted == 2],'g.')\n",
    "\n",
    "        plt.xlabel('date')\n",
    "        plt.ylabel('pef')\n",
    "        plt.title('PEF prediction ' + clsNames[i])\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
